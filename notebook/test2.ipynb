{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Removing top layer of efficient net and loading our own classification layer</h3>\n",
    "\n",
    "references:<br>\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/\">Tensorflow Documentation - Layers</a><br>\n",
    "<a href=\"https://arxiv.org/pdf/1905.11946.pdf\">Efficient Net and how it works</a><br>\n",
    "<a href=\"https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\">Keras example for fine tuning</a><br>\n",
    "<a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\">Tensorflow Documentation - Compiling and Evaluating</a><br>\n",
    "<a href=\"https://keras.io/api/optimizers/\">Keras Documentation - Optimisers</a><br>\n",
    "<a href=\"https://keras.io/api/metrics/\">Keras Documentation - Metrics</a><br>\n",
    "<a href=\"https://keras.io/api/losses/\">Keras Documentation - Losses</a><br>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0 as enet\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# from PIL import Image, ImageDraw\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import Model \n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "source": [
    "Preparing the Dataset\n",
    "We will first prepare the dataset and separate out the images:\n",
    "\n",
    "We first divide the folder contents into the train and validation directories.\n",
    "Then, in each of the directories, create a separate directory for cats that contains only cat images, and a separate director for dogs having only dog images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data'\n",
    "train_dir = os.path.join(base_dir, 'training')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "source": [
    "Step 1: Image Augmentation\n",
    "Since we took up a much smaller dataset of images earlier, we can make up for it by augmenting this data and increasing our dataset size. If you are working with the original larger dataset, you can skip this step and move straight on to building the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Step 2: Loading the Base Model\n",
    "We will be using the B0 version of EfficientNet since it is the simplest of the 8. I urge you to experiment with the rest of the models, though do keep in mind that the models go on becoming more and more complex, which might not be the best suited for a simple binary classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# loading pretrained model, setting input shape\n",
    "inputs = (224, 224, 3)\n",
    "\n",
    "# Selecting a topless model (sounds damn good...)\n",
    "basemodel = enet(include_top=False, input_shape=inputs, weights=\"imagenet\")\n",
    "\n",
    "# locking the trained weights (freezing?)\n",
    "for layer in basemodel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# checking out how its like\n",
    "basemodel.summary()"
   ]
  },
  {
   "source": [
    "Step 3: Build the model\n",
    "Just like Inceptionv3, we will perform these steps at the final layer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = basemodel.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_final = Model(basemodel.input, predictions)"
   ]
  },
  {
   "source": [
    "Step 4: Compile and Fit\n",
    "Let us again use the RMSProp Optimiser, though here, I have introduced a decay parameter:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(RMSprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "We finally fit the model on our data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.batch_losses = []\n",
    "    self.batch_acc = []\n",
    "\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    self.batch_losses.append(logs['loss'])\n",
    "    self.batch_acc.append(logs['accuracy'])\n",
    "    self.model.reset_metrics()\n",
    "\n",
    "batch_stats_callback = CollectBatchStats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n",
    "            )\n",
    "        )\n",
    "loss_and_error_printing_callback = LossAndErrorPrintingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_history = model_final.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 1, epochs = 10, callbacks=[batch_stats_callback])\n",
    "# eff_history = model_final.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: finetuning other layers of the pretrained model\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(batch_stats_callback.batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(batch_stats_callback.batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: fit the training set into the fine-tuned model to see if theres improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: process validation data and validate model with validation data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: write entry script for web api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: packing up the model (docker) and deploy (it will be a nightmare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: deploy model on cloud space, verify service is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: test model (and profit)\n"
   ]
  }
 ]
}